---
layout: post
title: "The Unnecessary Link: Are We the Weakest Chain in the Future of Coding?"
tags: AI, philosophy
---

<div style="text-align:center;">
<img align="center" src="https://alexgude.com/files/coding-llm/water_color_of_a_robot_writing_code.png" height="70%" width="70%"/>
</div>
<br/>

**When** I recently got access to GitHub Copilot _(with grace of UBS of course!)_, it felt like unlocking a secret weapon. Coding became less of a manual task and more of a collaborative experience. The lines of code appeared almost magically â€” suggested by an intelligent assistant that somehow knew what I was trying to do before I even did. It felt like the future.

But this experience triggered a deeper, more unsettling thought. 
> Are we, as developers, becoming the unnecessary link in the coding chain?

In the grand scheme of programming, where does the human truly fit? Today, we're an essential part of a chain: machines operate in binary, and we, the humans, convert ideas into readable code, which is then compiled back into binary for machines to execute. Itâ€™s a strange loop â€” computers talk to us in code, we write it down, and send it back through a compiler to talk to computers again.

The chain goes like this: the computer (with AI) creates or suggests code in a human-readable form, we humans review or modify that code, and then a compiler transforms it back into machine code. Essentially, we are the interpreter, the translator, the human middleware in this loop.

**Binary â†’ Human-readable code (via AI/IDE) â†’ Human (code-writer) â†’ Compiler â†’ Binary**

But what happens when the AI at the beginning of that chain becomes smart enough to skip us?

With tools like Copilot and GPT already capable of generating code with high accuracy, we're quickly shifting from being *code writers* to *code curators*. We describe the problem in plain English, and the AI proposes a solution. More often than not, that solution is spot on. And as these tools continue to learn, the need for us to even describe problems may shrink. The machine could infer requirements from Jira tickets, user feedback, monitoring systems, or even real-time data streams.

This opens the door to a future where the entire loop could be machine-to-machine. An AI could identify a problem, generate its own solution directly in binary or bytecode, deploy it, monitor it, and iterate â€” all without human involvement. The chain would now look like this: binary thought to binary instruction. Humans removed.

Why would this happen? Because, frankly, we are the bottleneck. We get tired, we get distracted, we argue over tabs vs spaces, and sometimes we even write bugs. AI doesnâ€™t. It doesnâ€™t forget, doesnâ€™t sleep, and doesnâ€™t push broken code because it was hungry or annoyed. From an efficiency standpoint, removing us makes perfect sense.

But rather than viewing this future with fear, we should see it as a transformation. In such a world, our role wouldnâ€™t be obsolete â€” it would be redefined. Instead of being code typists, we become problem framers, ethical stewards, experience designers, and AI coaches. We guide the intelligence rather than simulate it.

And in doing so, we might finally transcend the idea that we were meant to write code forever. Maybe our best contribution is not in writing the next great function, but in imagining the next great future.

If the machines stop waiting for us to write code, letâ€™s make sure weâ€™re busy doing something even more impactful â€” like building the world theyâ€™ll help us shape.

_Authored by a engineer who just realized their best line of code might be the last one they ever have to write._ ðŸ˜…ðŸ’»
